{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Arabic Service Data Preprocessing Pipeline\n",
    "\n",
    "This notebook documents and demonstrates the full preprocessing pipeline for Arabic government service data, as used in our rule-based, retrieval-based chatbot.\n",
    "\n",
    "The goal: **Standardize and enrich service data for accurate, robust search and retrieval.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 🧠 Why Preprocessing is Critical\n",
    "\n",
    "Arabic is morphologically rich and has many spelling variations (diacritics, hamza forms, etc).\n",
    "\n",
    "- **Without preprocessing:** User queries and service data may not match due to superficial differences.\n",
    "- **With preprocessing:** We standardize text, so queries like \"أريد تجديد الرخصه\" match services described as \"تجديد رخصة القيادة\".\n",
    "\n",
    "This pipeline ensures that retrieval is accurate and robust, even with spelling and morphological variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 📥 Input: Raw Service Data\n",
    "\n",
    "Each service entry (from scraping) contains fields like:\n",
    "\n",
    "- `category`\n",
    "- `service_name`\n",
    "- `description`\n",
    "- `terms`\n",
    "- `Documents`\n",
    "- `related_services`\n",
    "- `service_url`\n",
    "\n",
    "We focus on preprocessing the main textual fields: `service_name`, `description`, `terms`, and `Documents`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "If you haven't already, install the required libraries:\n",
    "\n",
    "```bash\n",
    "pip install camel_tools scikit-learn numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ca094019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\trying\\Najeeb_chatbot\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Manually resolve project root (e.g., two levels up from current notebook)\n",
    "ROOT_DIR = Path().resolve().parent  # adjust level as needed\n",
    "print(ROOT_DIR)\n",
    "sys.path.append(str(ROOT_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 2. Import Required Modules\n",
    "import re\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.normalize import normalize_unicode, normalize_alef_maksura_ar, normalize_alef_ar, normalize_teh_marbuta_ar\n",
    "from camel_tools.utils.dediac import dediac_ar\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Custom stopwords list (should cover all common Arabic stopwords)\n",
    "from preprocessing.stopwordsallforms import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9143e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data to test on it\n",
    "from typing import List\n",
    "from config import SCRAPED_SERVICES_FILE\n",
    "from scraping.scraper import ScrapedServiceData\n",
    "import json\n",
    "with open(SCRAPED_SERVICES_FILE, 'r', encoding='utf-8') as f:\n",
    "    data:List[ScrapedServiceData]  = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Text Normalization\n",
    "\n",
    "### What & Why\n",
    "\n",
    "- **What:** Standardize Arabic text by removing diacritics and normalizing different forms of Alef, Yaa, and Teh Marbuta.\n",
    "- **Why:** Reduces spelling variation, so different forms of the same word are treated as equal.\n",
    "\n",
    "This is crucial for matching queries to services, as users may type words in many different forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def norm(text):\n",
    "    \"\"\"\n",
    "    Normalize Arabic text by:\n",
    "    - Unicode normalization\n",
    "    - Removing diacritics\n",
    "    - Normalizing Alef, Yaa, Teh Marbuta\n",
    "    \"\"\"\n",
    "    normalized_text = normalize_unicode(text)\n",
    "    normalized_text = dediac_ar(normalized_text)\n",
    "    normalized_text = normalize_alef_ar(normalized_text)\n",
    "    normalized_text = normalize_alef_maksura_ar(normalized_text)\n",
    "    normalized_text = normalize_teh_marbuta_ar(normalized_text)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d3ee8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalization: تُمكّنك هذه الخدمة من تفعيل بطاقتك التموينية\n",
      "After normalization: تمكنك هذه الخدمه من تفعيل بطاقتك التموينيه\n"
     ]
    }
   ],
   "source": [
    "# Get the first service from data\n",
    "first_service = data[1]\n",
    "service_description = first_service['description']\n",
    "\n",
    "# Show before and after normalization\n",
    "print(\"Before normalization:\", service_description)\n",
    "print(\"After normalization:\", norm(service_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Tokenization & Stopword Removal\n",
    "\n",
    "### What & Why\n",
    "\n",
    "- **Tokenization:** Split text into words using `simple_word_tokenize` from CamelTools (handles Arabic morphology better than `split()`).\n",
    "- **Stopword Removal:** Remove common words (prepositions, pronouns, etc.) that don't help distinguish between services.\n",
    "- **Normalization:** Apply the `norm` function to each token.\n",
    "\n",
    "This step ensures that only meaningful, standardized words are kept for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - Remove punctuation\n",
    "    - Tokenize (CamelTools)\n",
    "    - Remove stopwords\n",
    "    - Normalize each token\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    return [norm(token) for token in tokens if len(token) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "f9327783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing: تُمكّنك هذه الخدمة من تفعيل بطاقتك التموينية\n",
      "After preprocessing: ['تمكنك', 'هذه', 'الخدمه', 'من', 'تفعيل', 'بطاقتك', 'التموينيه']\n"
     ]
    }
   ],
   "source": [
    "# Show before and after preprocessing\n",
    "print(\"Before preprocessing:\", service_description)\n",
    "print(\"After preprocessing:\", preprocess_text(service_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 5. Enrich Service Data with Text Fields\n",
    "\n",
    "### What & Why\n",
    "\n",
    "- **What:** For each service, create two new fields:\n",
    "    - `full_text`: Concatenation of all important fields (category, name, description, terms, documents)\n",
    "    - `short_text`: Concatenation of description, name, and category (used for keyword extraction)\n",
    "- **Why:**\n",
    "    - `full_text` provides rich context for training the TF-IDF model.\n",
    "    - `short_text` is a concise summary for extracting the most important keywords.\n",
    "\n",
    "This separation improves the quality of keyword extraction and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def enrich_services_with_texts(services_data):\n",
    "    \"\"\"\n",
    "    For each service, add normalized 'full_text' and 'short_text' fields.\n",
    "    \"\"\"\n",
    "    for service in services_data:\n",
    "        # Get each relevant field, defaulting to empty string if missing\n",
    "        category = service.get(\"category\", \"\")\n",
    "        name = service.get(\"service_name\", \"\")\n",
    "        desc = service.get(\"description\", \"\")\n",
    "        # Join terms list into a single string\n",
    "        terms = \" \".join(service.get(\"terms\", []))\n",
    "        documents = \" \".join(service.get(\"terms\", []))\n",
    "        # Concatenate all fields for full_text\n",
    "        full_text = f\"{category} {name} {desc} {terms} {documents}\"\n",
    "        service[\"full_text\"] = norm(full_text)\n",
    "        # Concatenate description, name, and category for short_text\n",
    "        service[\"short_text\"] = norm(f\"{category} {name} {desc}\")\n",
    "    return services_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6c42dbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text\n",
      " التموين تفعيل بطاقه تموين تمكنك هذه الخدمه من تفعيل بطاقتك التموينيه مالك البطاقه فقط (رب الاسره) المؤهل لطلب الخدمه يجب ان تكون البطاقه قد سلمت للمواطن مالك البطاقه فقط (رب الاسره) المؤهل لطلب الخدمه يجب ان تكون البطاقه قد سلمت للمواطن\n",
      "short_text\n",
      " التموين تفعيل بطاقه تموين تمكنك هذه الخدمه من تفعيل بطاقتك التموينيه\n"
     ]
    }
   ],
   "source": [
    "# Apply enrich_services_with_texts to a single service by wrapping it in a list\n",
    "enriched_first_service = enrich_services_with_texts([first_service])[0]\n",
    "print('full_text\\n', enriched_first_service[\"full_text\"])\n",
    "print('short_text\\n', enriched_first_service[\"short_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b574f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['التموين',\n",
       " 'تفعيل',\n",
       " 'بطاقه',\n",
       " 'تموين',\n",
       " 'تمكنك',\n",
       " 'هذه',\n",
       " 'الخدمه',\n",
       " 'من',\n",
       " 'تفعيل',\n",
       " 'بطاقتك',\n",
       " 'التموينيه']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(enriched_first_service[\"short_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 6. TF-IDF Keyword Extraction\n",
    "\n",
    "### What & Why\n",
    "\n",
    "- **What:** Use TF-IDF to extract the most distinctive keywords for each service.\n",
    "- **Why:**\n",
    "    - TF-IDF highlights words that are frequent in a service but rare across all services.\n",
    "    - This helps the chatbot match user queries to the most relevant services.\n",
    "\n",
    "We train the TF-IDF vectorizer on all `full_text` fields, then extract top keywords from each service's `short_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "43c9e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def category_level_full_texts(services_data):\n",
    "    # Group full_texts by category\n",
    "    category_to_texts = defaultdict(list)\n",
    "    for service in services_data:\n",
    "        category = service.get(\"category\", \"\")\n",
    "        category_to_texts[category].append(service[\"full_text\"])\n",
    "    # Create a mapping: category -> concatenated full_text\n",
    "    category_to_bigtext = {cat: \" \".join(texts) for cat, texts in category_to_texts.items()}\n",
    "    # For each service, assign the bigtext of its category\n",
    "    category_full_texts = [category_to_bigtext[service.get(\"category\", \"\")] for service in services_data]\n",
    "    return category_full_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cfbff1",
   "metadata": {},
   "source": [
    "- Each service in the same category will have the same (big) full_text, repeated for each service in that category.\n",
    "- The list length matches the number of services, so you can use the batch approach with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def extract_keywords(services_data:List[ScrapedServiceData], top_n=4):\n",
    "    \"\"\"\n",
    "    - Train TF-IDF on all full_texts (preprocessed).\n",
    "    - For each service, extract top N keywords from its short_text.\n",
    "    - Save keywords in service['keywords'].\n",
    "    \"\"\"\n",
    "    normalized_stopwords = [norm(word) for word in STOPWORDS.keys()]\n",
    "\n",
    "    # Fit vectorizer on all full_texts (list of strings)\n",
    "    category_full_texts = category_level_full_texts(services_data)\n",
    "    vectorizer = TfidfVectorizer(stop_words=normalized_stopwords, tokenizer=preprocess_text, token_pattern=None)\n",
    "    vectorizer.fit(category_full_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Transform all short_texts (list of strings)\n",
    "    short_texts = [service[\"short_text\"] for service in services_data]\n",
    "    services_matrix = vectorizer.transform(short_texts)  # shape: (n_services, n_features)\n",
    "\n",
    "    for idx, service in enumerate(services_data):\n",
    "        scores = services_matrix[idx].toarray().flatten()\n",
    "        top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "        keywords = [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "        service[\"keywords\"] = keywords\n",
    "    return services_data, vectorizer, services_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "da6f730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service 1: استمارة تحديث بيانات المواطن\n",
      "keywords: ['تحديث', 'المقدمه', 'جوده', 'الخدمات']\n",
      "Service 2: تفعيل بطاقة تموين\n",
      "keywords: ['تفعيل', 'بطاقتك', 'الخدمه', 'تمكنك']\n",
      "Service 3: إصدار بدل تالف أو فاقد لبطاقة تموين\n",
      "keywords: ['بدل', 'تالف', 'فاقد', 'اصدار']\n",
      "Service 4: نقل من محافظة إلى أخرى\n",
      "keywords: ['اخري', 'محافظه', 'نقل', 'التموينيه']\n",
      "Service 5: فصل نفسي\n",
      "keywords: ['فصل', 'الخدمه', 'الحاليه', 'بطاقه']\n",
      "Service 6: ضم أفراد أسرتى\n",
      "keywords: ['ضم', 'افراد', 'بطاقتك', 'الخدمه']\n",
      "Service 7: الاستعلام عن صرف\n",
      "keywords: ['الاستعلام', 'صرف', 'الخدمه', 'البطاقه']\n"
     ]
    }
   ],
   "source": [
    "# Use the first 3 services from the loaded data for testing\n",
    "test_services = data[:7]\n",
    "enriched_services = enrich_services_with_texts(test_services)\n",
    "services_data, vectorizer, services_matrix = extract_keywords(enriched_services, top_n=4)\n",
    "\n",
    "# Display keywords for each service\n",
    "for i, service in enumerate(services_data):\n",
    "    print(f\"Service {i+1}: {service['service_name']}\")\n",
    "    print(\"keywords:\", service[\"keywords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb91896",
   "metadata": {},
   "source": [
    "### Try the model using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "655edd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest service: تفعيل بطاقة تموين\n",
      "Similarity score: 0.4264014327112209\n",
      "Service description: تُمكّنك هذه الخدمة من تفعيل بطاقتك التموينية\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example: Find the closest service to a user query using the trained TF-IDF vectorizer and services_matrix\n",
    "\n",
    "def find_closest_service(query, vectorizer, services_matrix, services_data):\n",
    "    # Preprocess the query in the same way as service data\n",
    "    # Transform the query to TF-IDF vector\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    # Compute cosine similarity with all services\n",
    "    similarities = cosine_similarity(query_vec, services_matrix).flatten()\n",
    "    # Get the index of the most similar service\n",
    "    best_idx = similarities.argmax()\n",
    "    return services_data[best_idx], similarities[best_idx]\n",
    "\n",
    "# Example usage:\n",
    "user_query = \"عاوز افعل بطاقة التموين\"\n",
    "closest_service, score = find_closest_service(user_query, vectorizer, services_matrix, services_data)\n",
    "print(\"Closest service:\", closest_service[\"service_name\"])\n",
    "print(\"Similarity score:\", score)\n",
    "print(\"Service description:\", closest_service[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 🔍 Notes & Best Practices\n",
    "\n",
    "- Always preprocess both the service data and user queries in the **same way** for best retrieval accuracy.\n",
    "- For even better results, consider using morphological analysis (lemmatization) or part-of-speech filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7cfa8",
   "metadata": {},
   "source": [
    "# changes we made with time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81555d",
   "metadata": {},
   "source": [
    "- We grouped `full_text` by category, so for all services in the same category, we combined their `full_text` fields. This way, the TF-IDF model learns what is important for each category, since services in the same category often share similar words. This can improve keyword extraction and retrieval accuracy for services that belong to the same category.\n",
    "- we normalized full text and short text in production and store them, instead of storing the raw combination\n",
    "* We moved **tokenization and stopword removal into the vectorizer** pipeline for cleaner, more integrated preprocessing and feature extraction.\n",
    "* We **delegated stopword removal** to `TfidfVectorizer` by passing a **normalized stopword list** to its `stop_words` parameter or handled it inside the tokenizer (to avoid inconsistency).\n",
    "* We chose **word-level tokens (`analyzer='word'`)** with `ngram_range=(1,1)` (unigrams) for better similarity results, since it matches key Arabic words directly.\n",
    "- now we don't need to preprocess the query again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50141151",
   "metadata": {},
   "source": [
    "# some mistakes we were doing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd738714",
   "metadata": {},
   "source": [
    "### 📝 TF-IDF Keyword Extraction: Per-Service vs. Batch Approach\n",
    "\n",
    "#### **Old Approach: Per-Service Transformation**\n",
    "\n",
    "- **How it worked:**  \n",
    "  - Preprocess and join each service’s `full_text`, fit the vectorizer on all services (as a list).\n",
    "  - For each service, preprocess and join its `short_text`, transform it individually, and extract keywords from its TF-IDF vector.\n",
    "  - Example:\n",
    "    ```python\n",
    "    full_texts = [\" \".join(preprocess_text(service[\"full_text\"])) for service in services_data]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(full_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for service in services_data:\n",
    "        short_text = \" \".join(preprocess_text(service[\"short_text\"]))\n",
    "        tfidf_vector = vectorizer.transform([short_text])\n",
    "        scores = tfidf_vector.toarray().flatten()\n",
    "        top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "        keywords = [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "        service[\"keywords\"] = keywords\n",
    "    ```\n",
    "- **Pros:**  \n",
    "  - Works for per-service keyword extraction.\n",
    "  - Avoids indexing errors.\n",
    "- **Cons:**  \n",
    "  - Less efficient (calls `transform` for each service separately).\n",
    "  - More code repetition.\n",
    "\n",
    "---\n",
    "\n",
    "#### **New Approach: Batch Transformation (Recommended)**\n",
    "\n",
    "- **How it works:**  \n",
    "  - Fit the vectorizer on a list of all `full_text` fields (one per service).\n",
    "  - Transform all `short_text` fields at once as a batch (list of strings).\n",
    "  - Iterate over the resulting TF-IDF matrix (one row per service) to extract keywords.\n",
    "  - Example:\n",
    "    ```python\n",
    "    full_texts = [service[\"full_text\"] for service in services_data]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(full_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    short_texts = [service[\"short_text\"] for service in services_data]\n",
    "    tfidf_matrix = vectorizer.transform(short_texts)  # shape: (n_services, n_features)\n",
    "\n",
    "    for idx, service in enumerate(services_data):\n",
    "        scores = tfidf_matrix[idx].toarray().flatten()\n",
    "        top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "        keywords = [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "        service[\"keywords\"] = keywords\n",
    "    ```\n",
    "- **Pros:**  \n",
    "  - More efficient (vectorized, fewer function calls).\n",
    "  - Indexing is straightforward and safe.\n",
    "  - Preferred and idiomatic in scikit-learn.\n",
    "- **Cons:**  \n",
    "  - None significant for this use case.\n",
    "\n",
    "---\n",
    "\n",
    "#### Which Approach Gives Better Keyword Extraction?\n",
    "\n",
    "- **In theory:** Both approaches should give the same results if you preprocess texts identically and use the same vectorizer settings.\n",
    "- **In practice:** If you see different keywords, it's likely due to differences in how you preprocess or join the text before passing it to the vectorizer.\n",
    "    - Always ensure you use the same preprocessing and joining logic for both `full_text` and `short_text`.\n",
    "    - For best results, preprocess your text, join tokens with spaces, and pass the resulting strings as a list to both `fit()` and `transform()`.\n",
    "\n",
    "**Key Point:**  \n",
    "- Consistency in preprocessing is more important than the choice between per-service and batch transformation.  \n",
    "- The batch approach is still recommended for efficiency and clarity, but always double-check your preprocessing pipeline for consistency to ensure high-quality keyword extraction.\n",
    "---\n",
    "\n",
    "#### **Summary Table**\n",
    "\n",
    "| Approach         | How?                                 | Works? | Efficient? | Indexing Safe? | Keyword Quality | Recommended? |\n",
    "|------------------|--------------------------------------|--------|------------|----------------|-----------------|--------------|\n",
    "| Per-Service      | Transform each short_text separately | Yes    | No         | Yes            | Good            | OK           |\n",
    "| Batch            | Transform all short_texts at once    | Yes    | Yes        | Yes            | Good            | **Best**     |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway:**  \n",
    "Both approaches produce the same keywords, but the **batch approach** is more efficient, concise, and recommended for production code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51808695",
   "metadata": {},
   "source": [
    "# Old code compareson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f33f1e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_text\n",
      " التموين تفعيل بطاقة تموين تُمكّنك هذه الخدمة من تفعيل بطاقتك التموينية مالك البطاقة فقط (رب الأسرة) المؤهل لطلب الخدمة يجب أن تكون البطاقة قد سٌلًمت للمواطن []\n",
      "short_text\n",
      " تُمكّنك هذه الخدمة من تفعيل بطاقتك التموينية تفعيل بطاقة تموين التموين\n",
      "Service 1: استمارة تحديث بيانات المواطن\n",
      "keywords: ['تحديث', 'الجهه', 'بياناتهم', 'بيانات']\n",
      "Service 2: تفعيل بطاقة تموين\n",
      "keywords: ['تفعيل', 'بطاقتك', 'التموينيه', 'بطاقه']\n",
      "Service 3: إصدار بدل تالف أو فاقد لبطاقة تموين\n",
      "keywords: ['تالف', 'اصدار', 'فاقد', 'بدل']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    - Remove punctuation\n",
    "    - Tokenize (CamelTools)\n",
    "    - Remove stopwords\n",
    "    - Normalize each token\n",
    "    \"\"\"\n",
    "    # Remove all characters that are not word characters (letters, digits, or underscore) or whitespace.\n",
    "    # This strips out punctuation and special symbols, leaving only Arabic/English letters and spaces.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize using CamelTools (handles Arabic morphology)\n",
    "    tokens = simple_word_tokenize(text)\n",
    "    # Prepare set of Arabic stopwords\n",
    "    ar_stopwords = set(STOPWORDS.keys())\n",
    "    # Normalize each token and remove stopwords and single-character tokens\n",
    "    return [norm(token) for token in tokens if token not in ar_stopwords and len(token) > 1]\n",
    "\n",
    "def enrich_services_with_texts(services_data):\n",
    "    \"\"\"\n",
    "    For each service, add 'full_text' and 'short_text' fields.\n",
    "    Only include 'Documents' if not 'لا يوجد'.\n",
    "    \"\"\"\n",
    "    for service in services_data:\n",
    "        category = service.get(\"category\", \"\")\n",
    "        name = service.get(\"service_name\", \"\")\n",
    "        desc = service.get(\"description\", \"\")\n",
    "        terms = \" \".join(service.get(\"terms\", []))\n",
    "        documents = service.get(\"Documents\", \"\")\n",
    "        full_text = f\"{category} {name} {desc} {terms} {documents}\"\n",
    "        service[\"full_text\"] = full_text.strip()\n",
    "        service[\"short_text\"] = f\"{desc} {name} {category}\".strip()\n",
    "    return services_data\n",
    "# Apply enrich_services_with_texts to a single service by wrapping it in a list\n",
    "enriched_first_service = enrich_services_with_texts([first_service])[0]\n",
    "print('full_text\\n', enriched_first_service[\"full_text\"])\n",
    "print('short_text\\n', enriched_first_service[\"short_text\"])\n",
    "\n",
    "def extract_keywords_from_short_texts_with_vectorizer(services_data, top_n=4):\n",
    "    \"\"\"\n",
    "    - Train TF-IDF on all full_texts (preprocessed).\n",
    "    - For each service, extract top N keywords from its short_text.\n",
    "    - Save keywords in service['keywords'].\n",
    "    \"\"\"\n",
    "    full_texts = [\" \".join(preprocess_text(service[\"full_text\"])) for service in services_data]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(full_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    for service in services_data:\n",
    "        short_text = \" \".join(preprocess_text(service[\"short_text\"]))\n",
    "        tfidf_vector = vectorizer.transform([short_text])\n",
    "        scores = tfidf_vector.toarray().flatten()\n",
    "        top_indices = np.argsort(scores)[-top_n:][::-1]\n",
    "        keywords = [feature_names[i] for i in top_indices if scores[i] > 0]\n",
    "        service[\"keywords\"] = keywords\n",
    "    return services_data, vectorizer\n",
    "\n",
    "# Use the first 3 services from the loaded data for testing\n",
    "test_services = data[:3]\n",
    "enriched_services = enrich_services_with_texts(test_services)\n",
    "services_data, vectorizer = extract_keywords_from_short_texts_with_vectorizer(enriched_services, top_n=4)\n",
    "\n",
    "# Display keywords for each service\n",
    "for i, service in enumerate(services_data):\n",
    "    print(f\"Service {i+1}: {service['service_name']}\")\n",
    "    print(\"keywords:\", service[\"keywords\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
